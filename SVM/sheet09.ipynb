{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "In this exercise sheet, you will experiment with training various support vector machines on a subset of the MNIST dataset composed of digits 5 and 6. First, download the MNIST dataset from http://yann.lecun.com/exdb/mnist/, uncompress the downloaded files, and place them in a `data/` subfolder. Install the optimization library CVXOPT (`python-cvxopt` package, or directly from the website `www.cvxopt.org`). This library will be used to optimize the dual SVM in part A.\n",
    "\n",
    "## Part A: Kernel SVM and Optimization in the Dual\n",
    "\n",
    "We would like to learn a nonlinear SVM by optimizing its dual. An advantage of the dual SVM compared to the primal SVM is that it allows to use nonlinear kernels such as the Gaussian kernel, that we define as:\n",
    "$$\n",
    "k(x,x') = \\exp \\Big( -\\frac{\\|x-x'\\|^2}{\\sigma^2} \\Big)\n",
    "$$\n",
    "The dual SVM consists of solving the following quadratic program:\n",
    "$$\n",
    "\\max_\\alpha \\sum_{i=1}^N \\alpha_i - \\frac12 \\sum_{ij} \\alpha_i \\alpha_j y_i y_j k(x_i,x_j)\n",
    "$$\n",
    "subject to:\n",
    "$$0 \\leq \\alpha_i \\leq C \\qquad \\text{and} \\qquad \\sum_{i=1}^N \\alpha_i y_i = 0.$$\n",
    "\n",
    "Then, given the alphas, the prediction of the SVM can be obtained as:\n",
    "$$\n",
    "f(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 \\qquad & \\text{if} \\quad \\sum_{i=1}^N \\alpha_i y_i k(x,x_i) + \\theta > 0\\\\\n",
    "-1 \\qquad & \\text{if} \\quad \\sum_{i=1}^N \\alpha_i y_i k(x,x_i) + \\theta < 0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = \\frac{1}{\\# SV} \\sum_{i \\in SV} \\left( y_i - \\sum_{j=1}^N \\alpha_j y_j k(x_i,x_j) \\right)\n",
    "$$\n",
    "and `SV` is the set of indices corresponding to the unbound support vectors.\n",
    "\n",
    "### Implementation (25 P)\n",
    "\n",
    "We will solve the dual SVM applied to the MNIST dataset using the CVXOPT quadratic optimizer. For this, we have to build the data structures (vectors and matrices) to must be passed to the optimizer.\n",
    "\n",
    "* *Implement* a function `gaussianKernel` that returns for a Gaussian kernel of scale $\\sigma$, the Gram matrix of the two data sets given as argument.\n",
    "* *Implement* a function `getQPMatrices` that builds the matrices `P`, `q`, `G`, `h`, `A`, `b` (of type `cvxopt.matrix`) that need to be passed as argument to the optimizer `cvxopt.solvers.qp`.\n",
    "* *Run* the code below using the functions that you just implemented. (It should take less than 3 minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\htlsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale= 10  C=  1  SV: 1000  Train: 1.000  Test: 0.937 \n",
      "Scale= 10  C= 10  SV: 1000  Train: 1.000  Test: 0.937 \n",
      "Scale= 10  C=100  SV: 1000  Train: 1.000  Test: 0.937 \n",
      "\n",
      "Scale= 30  C=  1  SV:  254  Train: 1.000  Test: 0.985 \n",
      "Scale= 30  C= 10  SV:  274  Train: 1.000  Test: 0.986 \n",
      "Scale= 30  C=100  SV:  256  Train: 1.000  Test: 0.986 \n",
      "\n",
      "Scale=100  C=  1  SV:  317  Train: 0.973  Test: 0.971 \n",
      "Scale=100  C= 10  SV:  159  Train: 0.990  Test: 0.975 \n",
      "Scale=100  C=100  SV:  136  Train: 1.000  Test: 0.975 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils,numpy,cvxopt,cvxopt.solvers\n",
    "import time\n",
    "import scipy,scipy.spatial\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.getMNIST56()\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "\n",
    "def gaussianKernel(X1,X2,scale):\n",
    "    Dsq = scipy.spatial.distance.cdist(X1,X2,'euclidean')**2\n",
    "    return numpy.exp(-Dsq/scale**2)\n",
    "\n",
    "for scale in [10,30,100]:\n",
    "    for C in [1,10,100]:\n",
    "        \n",
    "        # Prepare kernel matrices\n",
    "\n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        Ktrain = gaussianKernel(Xtrain,Xtrain,scale)\n",
    "        Ktest  = gaussianKernel(Xtest,Xtrain,scale)\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # Prepare the matrices for the quadratic program\n",
    "        Y = Ttrain\n",
    "        N = y.shape[0]\n",
    "        P = numpy.outer(y,y)*Ktrain\n",
    "        q = -numpy.ones([N])\n",
    "        G = numpy.concatenate([numpy.identity(N),-numpy.identity(N)])\n",
    "        h = numpy.concatenate([C*numpy.ones([N]),numpy.zeros([N])])\n",
    "        A = Y.reshape([1,N])\n",
    "        b = numpy.zeros([1])\n",
    "        \n",
    "        P = cvxopt.matrix(P)\n",
    "        q = cvxopt.matrix(q)\n",
    "        G = cvxopt.matrix(G)\n",
    "        h = cvxopt.matrix(h)\n",
    "        A = cvxopt.matrix(A)\n",
    "        b = cvxopt.matrix(b)\n",
    "        \n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        #P,q,G,h,A,b = getQPMatrices(Ktrain,Ttrain,C)\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # Train the model (i.e. compute the alphas)\n",
    "        alpha = numpy.array(cvxopt.solvers.qp(P,q,G,h,A,b)['x']).flatten()\n",
    "        \n",
    "        # Get predictions for the training and test set\n",
    "        SV = (alpha>1e-6)\n",
    "        uSV = SV*(alpha<C-1e-6)\n",
    "        theta = 1.0/sum(uSV)*(Ttrain[uSV]-numpy.dot(Ktrain[uSV,:],alpha*Ttrain)).sum()\n",
    "        Ytrain = numpy.sign(numpy.dot(Ktrain[:,SV],alpha[SV]*Ttrain[SV])+theta)\n",
    "        Ytest  = numpy.sign(numpy.dot(Ktest [:,SV],alpha[SV]*Ttrain[SV])+theta)\n",
    "        \n",
    "        # Print accuracy and number of support vectors\n",
    "        Atrain = (Ytrain==Ttrain).mean()\n",
    "        Atest  = (Ytest ==Ttest ).mean()\n",
    "\n",
    "        t_end = time.clock()\n",
    "        t = t_end - t_ini\n",
    "        print('Scale=%3d  C=%3d  SV: %4d  Train: %.3f  Test: %.3f ' %(scale,C,sum(SV),Atrain,Atest))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy,scipy.spatial\n",
    "from cvxopt import matrix\n",
    "\n",
    "def gaussianKernel(X1,X2,sigma):\n",
    "    \n",
    "    X_dis = scipy.spatial.distance.cdist(X1,X2,'euclidean')\n",
    "    K = numpy.exp(numpy.power(X_dis,2)/(-sigma**2))\n",
    "    \n",
    "    return K\n",
    "\n",
    "def getQPMatrices(X,Y,C):\n",
    "    N = len(X)\n",
    "    \n",
    "    P = numpy.outer(Y,Y)*X\n",
    "    P = matrix(P)\n",
    "    q = -numpy.ones(N)\n",
    "    q = matrix(q)\n",
    "    G = -numpy.eye(N)\n",
    "    G = matrix(G)\n",
    "    h = numpy.zeros(N)\n",
    "    h = matrix(h)\n",
    "    A = Y.reshape(1,1000)\n",
    "    A = matrix(A)\n",
    "    b = numpy.zeros(1)\n",
    "    b = matrix(b)\n",
    "    \n",
    "    return P,q,G,h,A,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\htlsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\htlsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale= 10  C=  1  SV: 1000  Train: 1.000  Test: 0.937   Time:7.330\n",
      "Scale= 10  C= 10  SV: 1000  Train: 1.000  Test: 0.937   Time:7.116\n",
      "Scale= 10  C=100  SV: 1000  Train: 1.000  Test: 0.937   Time:7.023\n",
      "\n",
      "Scale= 30  C=  1  SV:  256  Train: 1.000  Test: 0.986   Time:7.246\n",
      "Scale= 30  C= 10  SV:  256  Train: 1.000  Test: 0.986   Time:7.326\n",
      "Scale= 30  C=100  SV:  256  Train: 1.000  Test: 0.986   Time:7.321\n",
      "\n",
      "Scale=100  C=  1  SV:  134  Train: 1.000  Test: 0.975   Time:7.366\n",
      "Scale=100  C= 10  SV:  134  Train: 1.000  Test: 0.975   Time:7.328\n",
      "Scale=100  C=100  SV:  134  Train: 1.000  Test: 0.975   Time:7.322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils,numpy,cvxopt,cvxopt.solvers\n",
    "import time\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.getMNIST56()\n",
    "\n",
    "cvxopt.solvers.options['show_progress'] = False\n",
    "\n",
    "for scale in [10,30,100]:\n",
    "    for C in [1,10,100]:\n",
    "        \n",
    "        t_ini = time.clock()\n",
    "        # Prepare kernel matrices\n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        Ktrain = gaussianKernel(Xtrain,Xtrain,scale)\n",
    "        Ktest  = gaussianKernel(Xtest,Xtrain,scale)\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # Prepare the matrices for the quadratic program\n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        P,q,G,h,A,b = getQPMatrices(Ktrain,Ttrain,C)\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        # Train the model (i.e. compute the alphas)\n",
    "        alpha = numpy.array(cvxopt.solvers.qp(P,q,G,h,A,b)['x']).flatten()\n",
    "        \n",
    "        # Get predictions for the training and test set\n",
    "        SV = (alpha>1e-6)\n",
    "        uSV = SV*(alpha<C-1e-6)\n",
    "        theta = 1.0/sum(uSV)*(Ttrain[uSV]-numpy.dot(Ktrain[uSV,:],alpha*Ttrain)).sum()\n",
    "        Ytrain = numpy.sign(numpy.dot(Ktrain[:,SV],alpha[SV]*Ttrain[SV])+theta)\n",
    "        Ytest  = numpy.sign(numpy.dot(Ktest [:,SV],alpha[SV]*Ttrain[SV])+theta)\n",
    "        \n",
    "        # Print accuracy and number of support vectors\n",
    "        Atrain = (Ytrain==Ttrain).mean()\n",
    "        Atest  = (Ytest ==Ttest ).mean()\n",
    "\n",
    "        t_end = time.clock()\n",
    "        t = t_end - t_ini\n",
    "        print('Scale=%3d  C=%3d  SV: %4d  Train: %.3f  Test: %.3f   Time:%.3f' %(scale,C,sum(SV),Atrain,Atest,t))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (10 P)\n",
    "\n",
    "* *Explain* which combinations of parameters $\\sigma$ and $C$ lead to good generalization, underfitting or overfitting?\n",
    "\n",
    " $Answer:$ As we can see from the results above, that the combination of $\\sigma = 30 $  and $C = 1,10,100$  lead to good generalization with underfitting. \n",
    " \n",
    " \n",
    "* *Explain* which combinations of parameters $\\sigma$ and $C$ produce the fastest classifiers (in terms of amount of computation needed at prediction time)?\n",
    "\n",
    " $Answer:$ We can simply print the time spent after each iteration. The combination of parameters $\\sigma = 10$ and $C = 100$ produce the fastest classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Linear SVMs and Gradient Descent in the Primal\n",
    "\n",
    "The quadratic problem of the dual SVM does not scale well with the number of data points. For large number of data points, it is generally more appropriate to optimize the SVM in the primal. The primal optimization problem for linear SVMs can be written as\n",
    "$$\\min_{w,\\theta} ||w||^2 + C \\sum_{i=1}^N \\xi_i \\qquad \\text{where} \\qquad \\forall_{i=1}^N: y_i (w \\cdot x_i+\\theta) \\geq 1-\\xi_i \\qquad \\text{and} \\qquad \\xi_i \\geq 0.$$\n",
    "It is common to incorporate the constraints directly into the objective and then minimizing the unconstrained objective\n",
    "$$\\qquad J(w,\\theta) = ||w||^2 +C \\sum_{i=1}^N \\max(0,1-y_i (w \\cdot x_i + \\theta))$$\n",
    "using simple gradient descent.\n",
    "\n",
    "### Implementation (15 P)\n",
    "\n",
    "* *Implement* the function `J` computing the objective $J(w,\\theta)$\n",
    "* *Implement* the function `DJ` computing the gradient of the objective $J(w,\\theta)$ with respect to the parameters $w$ and $\\theta$.\n",
    "* *Run* the code below using the functions that you just implemented. (It should take less than 1 minute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It=  0   J:  1000.000  Train: 0.471  Test: 0.482\n",
      "It=  5   J: 12842.675  Train: 0.961  Test: 0.958\n",
      "It= 10   J: 10383.703  Train: 0.972  Test: 0.961\n",
      "It= 15   J:  8696.281  Train: 0.975  Test: 0.963\n",
      "It= 20   J:  7456.274  Train: 0.976  Test: 0.966\n",
      "It= 25   J:  6515.556  Train: 0.977  Test: 0.968\n",
      "It= 30   J:  5824.883  Train: 0.982  Test: 0.968\n",
      "It= 35   J:  5308.056  Train: 0.987  Test: 0.968\n",
      "It= 40   J:  4917.158  Train: 0.986  Test: 0.967\n",
      "It= 45   J:  4604.659  Train: 0.987  Test: 0.967\n",
      "It= 50   J:  4346.176  Train: 0.990  Test: 0.968\n",
      "It= 55   J:  4107.620  Train: 0.989  Test: 0.966\n",
      "It= 60   J:  3924.791  Train: 0.993  Test: 0.966\n",
      "It= 65   J:  3765.598  Train: 0.996  Test: 0.966\n",
      "It= 70   J:  3644.448  Train: 0.997  Test: 0.967\n",
      "It= 75   J:  3570.045  Train: 0.995  Test: 0.967\n",
      "It= 80   J:  3469.536  Train: 0.999  Test: 0.966\n",
      "It= 85   J:  3398.402  Train: 1.000  Test: 0.966\n",
      "It= 90   J:  3331.043  Train: 1.000  Test: 0.966\n",
      "It= 95   J:  3265.018  Train: 1.000  Test: 0.966\n",
      "It=100   J:  3200.303  Train: 1.000  Test: 0.966\n"
     ]
    }
   ],
   "source": [
    "import utils,numpy\n",
    "\n",
    "C = 10.0\n",
    "lr = 0.001\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.getMNIST56()\n",
    "\n",
    "n,d = Xtrain.shape\n",
    "\n",
    "w = numpy.zeros([d])\n",
    "theta = 1e-9\n",
    "\n",
    "for it in range(0,101):\n",
    "    \n",
    "    # Monitor the training and test error every 5 iterations\n",
    "    if it%5==0:\n",
    "        Ytrain = numpy.sign(numpy.dot(Xtrain,w)+theta)\n",
    "        Ytest  = numpy.sign(numpy.dot(Xtest ,w)+theta)\n",
    "        \n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        Ztrain = numpy.dot(Xtrain,w)+theta\n",
    "        Obj = (w**2).sum() + numpy.maximum(0,1-Ttrain*Ztrain).sum()\n",
    "        #Obj = J(w,theta,C,Xtrain,Ttrain)\n",
    "        ###\n",
    "        \n",
    "        Etrain = (Ytrain==Ttrain).mean()\n",
    "        Etest  = (Ytest ==Ttest ).mean()\n",
    "        print('It=%3d   J: %9.3f  Train: %.3f  Test: %.3f'%(it,Obj,Etrain,Etest))\n",
    "        \n",
    "    Ztrain = numpy.dot(Xtrain,w)+theta\n",
    "    \n",
    "    tmp = -C * ((1- Ttrain*Ztrain)>0)\n",
    "    dw = 2*w + (tmp[:,numpy.newaxis] * Xtrain *Ttrain[:,numpy.newaxis]).sum(axis = 0)\n",
    "    dtheta = (C * ((1- Ttrain*Ztrain)>0) * Ttrain).sum()\n",
    "    ### TODO: REPLACE BY YOUR OWN CODE\n",
    "    #dw,dtheta = DJ(w,theta,C,Xtrain,Ttrain)\n",
    "    ###\n",
    "    \n",
    "    w = w - lr*dw\n",
    "    theta = theta - lr*dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(w,theta,C,X,Y):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    w_n = numpy.linalg.norm(w)\n",
    "    C_c = numpy.ones([N]) - Y*(numpy.dot(X,w)-theta*numpy.ones([N]))\n",
    "    C_c = C_c.reshape(N,1)\n",
    "    m = numpy.amax(numpy.concatenate((C_c,numpy.zeros([N,1])),axis = 1),axis = 1)\n",
    "    \n",
    "    J = w_n**2 + C*m.sum()\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "def DJ(w,theta,C,X,Y):\n",
    "    N,D = X.shape\n",
    "    \n",
    "    indicator = ((numpy.ones(N) - Y*(numpy.dot(X,w)+theta*numpy.ones(N)))>numpy.zeros(N))*1.0\n",
    "    \n",
    "    dw = 2*w - C*(((indicator*Y)[:,numpy.newaxis]*X).sum(axis=0))\n",
    "    dtheta = -C*(indicator*Y).sum(axis = 0)\n",
    "    \n",
    "    return dw,dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DJ(w,theta,C,X,Y):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    C_c = numpy.ones([N]) - Y*(numpy.dot(X,w)-theta*numpy.ones([N]))\n",
    "    C_c = C_c.reshape(N,1)\n",
    "    index = numpy.where(C_c > 0.0)\n",
    "    m = -Y[:,numpy.newaxis]*X\n",
    "    m = m[index]\n",
    "    t = -Y*theta\n",
    "    t = t.reshape(N,1)\n",
    "    t = t[index]\n",
    "    \n",
    "    dw = 2*w + C*m.sum(axis = 0)\n",
    "    dtheta = C*t.sum(axis = 0)\n",
    "    \n",
    "    return dw,dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It=  0   J: 10000.000  Train: 0.471  Test: 0.482\n",
      "It=  5   J: 68686.731  Train: 0.961  Test: 0.958\n",
      "It= 10   J: 50000.274  Train: 0.973  Test: 0.961\n",
      "It= 15   J: 37457.648  Train: 0.973  Test: 0.963\n",
      "It= 20   J: 28652.040  Train: 0.974  Test: 0.965\n",
      "It= 25   J: 21727.090  Train: 0.977  Test: 0.967\n",
      "It= 30   J: 16913.518  Train: 0.980  Test: 0.968\n",
      "It= 35   J: 13590.484  Train: 0.986  Test: 0.967\n",
      "It= 40   J: 11119.869  Train: 0.986  Test: 0.967\n",
      "It= 45   J:  9172.561  Train: 0.991  Test: 0.967\n",
      "It= 50   J:  7652.186  Train: 0.990  Test: 0.968\n",
      "It= 55   J:  6418.409  Train: 0.988  Test: 0.966\n",
      "It= 60   J:  5248.271  Train: 0.995  Test: 0.966\n",
      "It= 65   J:  4523.320  Train: 0.992  Test: 0.967\n",
      "It= 70   J:  4033.051  Train: 0.996  Test: 0.966\n",
      "It= 75   J:  3677.583  Train: 0.997  Test: 0.965\n",
      "It= 80   J:  3526.082  Train: 0.998  Test: 0.966\n",
      "It= 85   J:  3404.280  Train: 1.000  Test: 0.966\n",
      "It= 90   J:  3336.804  Train: 1.000  Test: 0.966\n",
      "It= 95   J:  3270.665  Train: 1.000  Test: 0.966\n",
      "It=100   J:  3205.837  Train: 1.000  Test: 0.966\n"
     ]
    }
   ],
   "source": [
    "import utils,numpy\n",
    "\n",
    "C = 10.0\n",
    "lr = 0.001\n",
    "\n",
    "Xtrain,Ttrain,Xtest,Ttest = utils.getMNIST56()\n",
    "\n",
    "n,d = Xtrain.shape\n",
    "\n",
    "w = numpy.zeros([d])\n",
    "theta = 1e-9\n",
    "\n",
    "for it in range(0,101):\n",
    "    \n",
    "    # Monitor the training and test error every 5 iterations\n",
    "    if it%5==0:\n",
    "        Ytrain = numpy.sign(numpy.dot(Xtrain,w)+theta)\n",
    "        Ytest  = numpy.sign(numpy.dot(Xtest ,w)+theta)\n",
    "        \n",
    "        ### TODO: REPLACE BY YOUR OWN CODE\n",
    "        Obj    = J(w,theta,C,Xtrain,Ttrain)\n",
    "        ###\n",
    "        \n",
    "        Etrain = (Ytrain==Ttrain).mean()\n",
    "        Etest  = (Ytest ==Ttest ).mean()\n",
    "        print('It=%3d   J: %9.3f  Train: %.3f  Test: %.3f'%(it,Obj,Etrain,Etest))\n",
    "\n",
    "    ### TODO: REPLACE BY YOUR OWN CODE\n",
    "    dw,dtheta = DJ(w,theta,C,Xtrain,Ttrain)\n",
    "    ###\n",
    "    \n",
    "    w = w - lr*dw\n",
    "    theta = theta - lr*dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
